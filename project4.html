<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Julie Chung</title>
  <link href="https://fonts.googleapis.com/css2?family=Karla:wght@300;500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>

<body>


<div class="top-bar">
    <div class="title">
        <a href="index.html" class="title-link">Julie Chung</a> 
    </div>
    <button class="hamburger" id="hamburger-btn" aria-label="Toggle menu">☰</button>
    <nav class="nav-links" id="nav-links">
        <a href="about.html">About</a>
        <a title="Julie Chung" href="assets/Heesu_Julie_Chung_Resume.pdf" target="_blank">Resume</a>
    </nav>
</div>


  <div id="progress-bar"></div>

<div class="project-image">
  <img src="assets/p4-thumbnail.png" alt="COMET: Composite Objective Modeling for Enhanced Task-Solving Header Image">
</div>

<div class="page-layout">
  

  <div class="sidebar">
    <a href="#abstract fade-in" class="sidebar-link">Abstract</a>
    <a href="#overview fade-in" class="sidebar-link">Overview</a>
    <a href="#problem" class="sidebar-link">Defining the Problem</a>
    <a href="#solution" class="sidebar-link">Proposing a Solution</a>
    <a href="#setup" class="sidebar-link">Environment Setup</a>
    <a href="#final" class="sidebar-link">Final Paper</a>
    <a href="#conclusion" class="sidebar-link">Conclusion</a>
  </div>

  <div class="main-content fade-in" style="animation-delay: 0.4s;">
    <h1>COMET: Composite Objective Modeling for Enhanced Task-Solving</h1>

    <div class="context">
      <div class="context-block">
        <div class="context-header"><b>Team</b></div>
        <div class="context-para">Julie Chung, Aryan Singh, Arjun Prakash, Nora Ayanian</div>
      </div>
      <div class="context-block">
        <div class="context-header"><b>Role</b></div>
        <div class="context-para">Undergraduate Researcher</div>
      </div>
      <div class="context-block">
        <div class="context-header"><b>Tools</b></div>
        <div class="context-para">JAX, Gymnax, Colab</div>
      </div>
      <div class="context-block">
        <div class="context-header"><b>Duration</b></div>
        <div class="context-para">January - May 2024</div>
      </div>
    </div>

    <h2 id="abstract fade-in" style="animation-delay: 0.4s;">Abstract</h2>
    <p>Large Language Models (LLMs) such as GPT-4 have extended their 
      ability beyond text generation to complex tasks such as the design of 
      re-ward functions for agent-based task execution. While human engineers
       are capable of designing these functions to train agents to complete tasks, 
       LLMs offer a scalable alternative for generating efficient algorithms. 
       However, adapting these pre-trained agents to navigate new environments 
       or undertake more complex missions remains a challenge. To address this, 
       we introduce COMET: Composite Objective Modeling for Enhanced Task-solving, a
      method leveraging the in-context learning capabilities of LLMs to enhance 
      agent adaptability. This paper demonstrates how COMET allows a point 
      robot to follow a certain trajectory to reach a goal, us- ing reward 
      functions autonomously generated by LLMs. Our approach not only showcases 
      the adaptability of agents in varied environments but also underscores the 
      potential of LLMs to handle increasingly complex decision-making scenarios. 
      Through COMET, we aim to set a new standard in agent flexibility and 
      performance in dynamic settings.</p>

    <h2 id="overview fade-in" style="animation-delay: 0.4s;">Overview</h2>
    <p>Autonomous agents have demonstrated proficiency in tasks 
      such as text generation, computation, and manipulation.
      Yet, adapting these agents to new objectives often requires
      manually crafting reward functions and gathering task-specific
      datasets-an expensive and time-intensive process. Recent advances 
      in using Large Language Models (LLMs), particularly GPT-4-Turbo, 
      offer a promising solution by generating reward functions in a scalable,
      zero-shot manner. This project explores how LLMs can not only accelerate
      reward function design, but also enable agents to transition between
      subtly different behaviors shaped by complex, evolving objectives.
      This project was completed under Professor Nora Ayanian and Mentor Arjun Prakash
      at Brown University School of Engineering.</p>

    <div class="case-image-container fade-in" style="animation-delay: 0.4s;">
        <img src="assets/comet-gif.gif" alt="COMET point robot environment demo gif">
    </div>


    <h2 id="problem">Defining the Problem</h2>
    <p>While LLMs have made reward specification more accessible, 
      most current approaches focus on single, predefined goals 
      dictated by the environment. These strategies lack the 
      flexibility to support composite reward modeling—where 
      an agent must balance multiple, possibly conflicting, 
      objectives. For example, prior work has used LLMs for 
      robotic planning (Yu et al., 2023) or to meet static 
      environmental goals (Song et al., 2023), but little research 
      has addressed how agents can generalize across tasks or adapt 
      to new constraints using a unified reward signal. Without
      the ability to integrate diverse goals, autonomous agents 
      remain limited in real-world adaptability.
    </p>

    <h2 id="solution">Proposing a Solution</h2>
    <p>To address this gap, we introduce COMET 
      (Composite Objective Modeling for Enhanced Task-solving), 
      a framework that leverages LLMs to design and refine composite 
      reward functions across multiple learning stages:
    </p>

    <p>
      <b>Initial Reward Modeling:</b> COMET uses GPT-4-Turbo to design a general-purpose reward function based on the environment and task description.</p>
    <p>
      <b>Inverted Reward Generation:</b> The LLM then generates a new reward function that deliberately contradicts or diverges from the original objective by leveraging the environment’s source code as state context.</p>
    <p>
      <b>Checkpointed Retraining:</b> The agent is trained using this alternate reward, and its learned parameters are saved to serve as initialization for future adaptations.</p>
    <p>
      <b>Composite Reward Integration:</b> Finally, COMET prompts the LLM to create a composite reward function that fuses both objectives. The agent is retrained on this combined reward using the saved parameters, allowing it to perform tasks with more nuanced and adaptable behavior.</p>
    
    <p>Despite some challenges, this iterative approach highlights the potential of LLMs to generalize reward learning, enabling autonomous agents to transition between complex, multi-objective tasks without retraining from scratch.</p>
    
    <h2 id="setup">Environment Setup</h2>

    <p>We used the Gymnax Point Robot environment, where the agent (a point robot) navigates a 2D space toward a target location. The environment state consists of two components: the robot's position and the goal's position. The robot selects actions represented as 2D vectors, with their magnitudes clipped to a maximum force to keep movements realistic.
      The environment supports two types of reward functions:
    </p>

    <p>
      A <b>dense reward</b> that provides continuous feedback based on the distance to the goal
    </p>

    <p>
      A <b>sparse reward</b> that only gives a reward once the robot reaches within a specific radius of the goal
    </p>

    <div class="case-image-container fade-in" style="animation-delay: 0.4s;">
        <img src="assets/reach_goal_rect_0418.gif" alt="COMET Point Robot after training gif">
    </div>

    <div class="case-image-container fade-in" style="animation-delay: 0.4s;">
        <img src="assets/reactangle-rendition3.png" alt="COMET Reward Plot">
    </div>




    <h2 id="final">Final Paper</h2>
    <p>Please refer to the link below to access the final paper and GitHub for this project.</p>

    <iframe 
      src="assets/COMET_Composite_Objective_Modeling_For_Enhanced_Task_Solving.pdf" 
      alt="COMET Final Paper"
      width="100%" 
      height="600px" 
      style="border: 1px solid black; border-radius: 10px;">
    </iframe>
    <div class="button-container">
      <a href="assets/COMET_Composite_Objective_Modeling_For_Enhanced_Task_Solving.pdf" target="_blank">
            <button class="redirect-button">Go to Final Paper</button>
      </a>
    </div>

    <div class="button-container">
      <a href="https://github.com/jjchilling/COMET-Composite-Objective-Modeling-for-Enhanced-Task-solving" target="_blank">
            <button class="redirect-button">Go to GitHub</button>
      </a>
    </div>

    <h2 id="conclusion">Conclusion</h2>
    <p>
      Overall, this project provided a valuable opportunity to explore the application of
       reinforcement learning in multi-agent task-solving scenarios. 
       Throughout the process, I developed a range of research skills—from 
       conducting preliminary investigations and refining a focused research question 
       to implementing state-of-the-art reinforcement learning frameworks such as JAX and Gymnax. 
       I will be continuing my work in this area as part of ongoing multi-agent systems 
       research under Professor Nora Ayanian at Brown University's School of Engineering in the following
       semesters.
    </p>
  </div>

</div>
  <footer class="footer">
      <div class="footer-icons">
          <a href="https://www.linkedin.com/in/julie-hee-su-chung" target="_blank" aria-label="LinkedIn">
      
          <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" fill="currentColor" viewBox="0 0 24 24">
              <path d="M19 0h-14C2.24 0 0 2.24 0 5v14c0 2.76 2.24 5 5 5h14c2.76 
              0 5-2.24 5-5V5c0-2.76-2.24-5-5-5zM7.34 19H4.66V9h2.68v10zM6 
              7.59c-.86 0-1.56-.7-1.56-1.56C4.44 5.17 5.14 4.5 6 4.5s1.56.67 
              1.56 1.53c0 .86-.7 1.56-1.56 1.56zM19 19h-2.66v-5.34c0-1.27-.03-2.9-1.77-2.9-1.78 
              0-2.06 1.39-2.06 2.81V19h-2.66V9h2.56v1.36h.04c.36-.68 
              1.26-1.4 2.6-1.4 2.78 0 3.3 1.83 3.3 4.2V19z"/>
          </svg>
          </a>

          <a href="https://github.com/jjchilling" target="_blank" aria-label="GitHub">
        
          <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" fill="currentColor" viewBox="0 0 24 24">
              <path d="M12 .5C5.65.5.5 5.65.5 12c0 5.09 3.29 9.41 7.84 
              10.94.57.1.78-.25.78-.56v-1.94c-3.19.69-3.87-1.54-3.87-1.54-.52-1.32-1.28-1.67-1.28-1.67-1.05-.72.08-.7.08-.7 
              1.16.08 1.77 1.19 1.77 1.19 1.03 1.77 2.7 1.26 
              3.36.96.1-.75.4-1.26.72-1.55-2.55-.29-5.23-1.27-5.23-5.63 
              0-1.24.44-2.26 1.16-3.06-.12-.29-.5-1.45.1-3.02 
              0 0 .95-.31 3.1 1.17a10.78 10.78 0 0 1 5.64 0c2.15-1.48 
              3.1-1.17 3.1-1.17.6 1.57.22 2.73.1 3.02.72.8 
              1.16 1.82 1.16 3.06 0 4.37-2.69 5.34-5.25 
              5.62.41.35.77 1.04.77 2.1v3.12c0 .31.2.67.79.56C20.71 
              21.41 24 17.09 24 12c0-6.35-5.15-11.5-12-11.5z"/>
          </svg>
          </a>
      </div>
      <div class="footer-text">© 2025 Julie Chung</div>
  </footer>

<script src="script.js"></script>
<script src="scroll.js"></script>
</body>
</html>
